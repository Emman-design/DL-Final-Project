{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Final Proj.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPTAuwuMUEkl0RwqbNWdFHJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AawBpJ_Pf4pv",
        "outputId": "1a9fc8b2-7901-4d3a-af87-1c3bfe1d9008"
      },
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "def openFile(file): \n",
        "    '''\n",
        "    Description: Read in text file from CLI and return the input string and label in seperate arrays\n",
        "    Input: file \n",
        "    Output: Two arrays, input and target, containing the input and target strings respectively \n",
        "    '''\n",
        "    input = []\n",
        "    target = []\n",
        "    with open(file, 'r') as f:\n",
        "        for line in f: \n",
        "            if ',' in line:\n",
        "                currentLine = line.split(',')\n",
        "                input.append(currentLine[0])\n",
        "                target.append(currentLine[1].strip('\\n'))\n",
        "            else:\n",
        "                input.append(currentLine)\n",
        "    return input, target\n",
        "\n",
        "def writeFile(input, output):\n",
        "    '''\n",
        "    Description: write the input and predicted translation to a text file\n",
        "    Input: Original input to model\n",
        "    Output: Output of model\n",
        "    Return: None, text file stored in file path where code is exectued in CLI\n",
        "    '''\n",
        "    with open('result_predict.txt', 'a') as f:\n",
        "        for i in input: \n",
        "            f.write(input[i])\n",
        "            f.write(',')\n",
        "            f.write(output[i])\n",
        "            f.write('\\n')\n",
        "    return\n",
        "\n",
        "def pairData(input, output): \n",
        "\n",
        "    source = Sequence()\n",
        "    target = Sequence()\n",
        "\n",
        "    pairs = []\n",
        "\n",
        "    for i in range(len(input)):\n",
        "        full = [input[i], output[i]]\n",
        "        source.addWord(input[i])\n",
        "        target.addWord(output[i])\n",
        "\n",
        "        pairs.append(full)\n",
        "\n",
        "    return source, target, pairs\n",
        "\n",
        "\n",
        "'''\n",
        "This section pre-processes the input and output sequences. We break the words down into their corresponding letters\n",
        "and feed the dicitionary of letters to the model. We convert each letter into a one-hot vector. \n",
        "'''\n",
        "sos_tkn = 0\n",
        "eos_tkn = 1\n",
        "\n",
        "class Sequence:\n",
        "    '''\n",
        "    Class to help make a dictionary. Each word from the input or target array is split into letters\n",
        "    and then added to the dictionary. Each letter is added to an index\n",
        "    '''\n",
        "    def __init__(self): \n",
        "    #initialize containers to hold the letters and corresponding index\n",
        "        self.letter2index = {}\n",
        "        self.letter2count = {}\n",
        "        self.index2letter = {}\n",
        "        self.n_letters = 2\n",
        "    \n",
        "    def addWord(self, word): \n",
        "         '''\n",
        "         Description: split a word into letters and pass to addLetter function\n",
        "         Input: word from input or target sequence\n",
        "         Output: none \n",
        "         '''\n",
        "         for letter in word:\n",
        "            self.addLetter(letter)\n",
        "     \n",
        "    def addLetter(self, letter): \n",
        "         if letter not in self.letter2index:\n",
        "             self.letter2index[letter] = self.n_letters\n",
        "             self.letter2count[letter] = 1\n",
        "             self.index2letter[self.n_letters] = letter\n",
        "             self.n_letters += 1\n",
        "         else:\n",
        "             self.letter2count[letter] += 1\n",
        "    \n",
        "'''\n",
        "This section converts strings to tensors and then makes them pairs \n",
        "'''\n",
        "\n",
        "def indexesFromWord(Sequence, word):\n",
        "    return [Sequence.letter2index[letter] for letter in word]\n",
        "\n",
        "def tensorFromWord(Sequence, word):\n",
        "    indexes = indexesFromWord(Sequence, word)\n",
        "    indexes.append(eos_tkn)\n",
        "\n",
        "    return torch.tensor(indexes, dtype = torch.long, device = device).view(-1,1)\n",
        "\n",
        "def tensorsFromPair(input_seq, target_seq, pair):\n",
        "\n",
        "    input_tensor = tensorFromWord(input_seq, pair[0])\n",
        "    target_tensor = tensorFromWord(target_seq, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "\n",
        "'''\n",
        "Define the model here \n",
        "'''\n",
        "\n",
        "class Encoder(nn.Module): \n",
        "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
        "       super(Encoder, self).__init__()\n",
        "      \n",
        "       self.input_dim = input_dim\n",
        "       self.embbed_dim = embbed_dim\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.num_layers = num_layers\n",
        "\n",
        "       self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
        "\n",
        "       self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "              \n",
        "    def forward(self, src):\n",
        "      \n",
        "       embedded = self.embedding(src).view(1,1,-1)\n",
        "       outputs, hidden = self.gru(embedded)\n",
        "       return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "   def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
        "       super(Decoder, self).__init__()\n",
        "\n",
        "       self.embbed_dim = embbed_dim\n",
        "       self.hidden_dim = hidden_dim\n",
        "       self.output_dim = output_dim\n",
        "       self.num_layers = num_layers\n",
        "\n",
        "       self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
        "       self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "       self.out = nn.Linear(self.hidden_dim, output_dim)\n",
        "       self.softmax = nn.LogSoftmax(dim=1)\n",
        "      \n",
        "   def forward(self, input, hidden):\n",
        "\n",
        "       input = input.view(1, -1)\n",
        "       embedded = F.relu(self.embedding(input))\n",
        "       output, hidden = self.gru(embedded, hidden)       \n",
        "       prediction = self.softmax(self.out(output[0]))\n",
        "      \n",
        "       return prediction, hidden\n",
        "\n",
        "'''\n",
        "Combine Encoder and Decoder\n",
        "'''\n",
        "class Seq2Seq(nn.Module):\n",
        "   def __init__(self, encoder, decoder, device):\n",
        "       super().__init__()\n",
        "      \n",
        "       self.encoder = encoder\n",
        "       self.decoder = decoder\n",
        "       self.device = device\n",
        "     \n",
        "   def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
        "\n",
        "       input_length = source.size(0) \n",
        "       batch_size = target.shape[1] \n",
        "       target_length = target.shape[0]\n",
        "       vocab_size = self.decoder.output_dim\n",
        "\n",
        "       outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "       for i in range(input_length):\n",
        "           encoder_output, encoder_hidden = self.encoder(source[i])\n",
        "\n",
        "\n",
        "       decoder_hidden = encoder_hidden.to(device)\n",
        "\n",
        "       decoder_input = torch.tensor([sos_tkn], device=device)  # SOS\n",
        "\n",
        "\n",
        "       for t in range(target_length):   \n",
        "           decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "           outputs[t] = decoder_output\n",
        "           teacher_force = random.random() < teacher_forcing_ratio\n",
        "           topv, topi = decoder_output.topk(1)\n",
        "           input = (target[t] if teacher_force else topi)\n",
        "           if(teacher_force == False and input.item() == eos_tkn):\n",
        "               break\n",
        "\n",
        "       return outputs\n",
        "\n",
        "tf_ratio = 0.5\n",
        "\n",
        "'''\n",
        "Loss calculation\n",
        "'''\n",
        "\n",
        "def lossCalc(model, input_tensor, target_tensor, optimizer, criterion):\n",
        "    optimizer.zero_grad()\n",
        "    input_length = input_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "    epoch_loss = 0 \n",
        "\n",
        "    output = model(input_tensor, target_tensor)\n",
        "\n",
        "    num_iter = output.size(0)\n",
        "\n",
        "    for i in range(num_iter): \n",
        "        loss += criterion(output[i], target_tensor[i])\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss = loss.item()/num_iter\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "'''\n",
        "Function for training the model\n",
        "'''\n",
        "\n",
        "def trainModel(model, source, target, pairs, num_iteration = 20000):\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "    criterion = nn.NLLLoss()\n",
        "    total_loss_iterations = 0\n",
        "\n",
        "    training_pairs = [tensorsFromPair(source, target, random.choice(pairs))\n",
        "                        for i in range(num_iteration)]\n",
        "\n",
        "    for iter in range(1, num_iteration+1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = lossCalc(model, input_tensor, target_tensor, optimizer, criterion)\n",
        "\n",
        "        total_loss_iterations += loss\n",
        "\n",
        "        if iter % 100 == 0: \n",
        "            print('%d training iteraitions completed' % (iter))\n",
        "\n",
        "        if iter % 5000 == 0:\n",
        "            avarage_loss= total_loss_iterations / 5000\n",
        "            total_loss_iterations = 0\n",
        "            print('%d %.4f' % (iter, avarage_loss))\n",
        "            \n",
        "    torch.save(model.state_dict(), 'trained_model.pt')\n",
        "    return model\n",
        "\n",
        "'''\n",
        "Setup for evaluating model\n",
        "'''\n",
        "def evaluate(model, input_lang, output_lang, words):\n",
        "   with torch.no_grad():\n",
        "       input_tensor = tensorFromWord(input_lang, words[0])\n",
        "       output_tensor = tensorFromWord(output_lang, words[1])\n",
        "  \n",
        "       decoded_words = []\n",
        "  \n",
        "       output = model(input_tensor, output_tensor)\n",
        "       # print(output_tensor)\n",
        "  \n",
        "       for ot in range(output.size(0)):\n",
        "           topv, topi = output[ot].topk(1)\n",
        "           # print(topi)\n",
        "\n",
        "           if topi[0].item() == eos_tkn:\n",
        "               decoded_words.append('<EOS>')\n",
        "               break\n",
        "           else:\n",
        "               decoded_words.append(output_lang.index2letter[topi[0].item()])\n",
        "   return decoded_words\n",
        "\n",
        "def evaluateRandomly(model, source, target, pairs, n=10):\n",
        "   for i in range(n):\n",
        "       pair = random.choice(pairs)\n",
        "       print('source {}'.format(pair[0]))\n",
        "       print('target {}'.format(pair[1]))\n",
        "       output_letters = evaluate(model, source, target, pair)\n",
        "       output_words = ' '.join(output_letters)\n",
        "       print('predicted {}'.format(output_words))\n",
        "\n",
        "\n",
        "#input, output = openFile(sys.argv[1])\n",
        "\n",
        "input, output = openFile('data_train.txt')\n",
        "\n",
        "source, target, pairs = pairData(input, output)\n",
        "\n",
        "randomize = random.choice(pairs)\n",
        "print('random sentence {}'.format(randomize))\n",
        "\n",
        "#print number of letters\n",
        "input_size = source.n_letters\n",
        "output_size = target.n_letters\n",
        "print('Input : {} Output : {}'.format(input_size, output_size))\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "\n",
        "encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
        "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "#print model \n",
        "print(encoder)\n",
        "print(decoder)\n",
        "\n",
        "\n",
        "model = trainModel(model, source, target, pairs)\n",
        "evaluateRandomly(model, source, target, pairs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "random sentence ['GILGFVFTL', 'CASSTGRNYGYTF']\n",
            "Input : 22 Output : 22\n",
            "Encoder(\n",
            "  (embedding): Embedding(22, 256)\n",
            "  (gru): GRU(256, 512)\n",
            ")\n",
            "Decoder(\n",
            "  (embedding): Embedding(22, 256)\n",
            "  (gru): GRU(256, 512)\n",
            "  (out): Linear(in_features=512, out_features=22, bias=True)\n",
            "  (softmax): LogSoftmax(dim=1)\n",
            ")\n",
            "100 training iteraitions completed\n",
            "200 training iteraitions completed\n",
            "300 training iteraitions completed\n",
            "400 training iteraitions completed\n",
            "500 training iteraitions completed\n",
            "600 training iteraitions completed\n",
            "700 training iteraitions completed\n",
            "800 training iteraitions completed\n",
            "900 training iteraitions completed\n",
            "1000 training iteraitions completed\n",
            "1100 training iteraitions completed\n",
            "1200 training iteraitions completed\n",
            "1300 training iteraitions completed\n",
            "1400 training iteraitions completed\n",
            "1500 training iteraitions completed\n",
            "1600 training iteraitions completed\n",
            "1700 training iteraitions completed\n",
            "1800 training iteraitions completed\n",
            "1900 training iteraitions completed\n",
            "2000 training iteraitions completed\n",
            "2100 training iteraitions completed\n",
            "2200 training iteraitions completed\n",
            "2300 training iteraitions completed\n",
            "2400 training iteraitions completed\n",
            "2500 training iteraitions completed\n",
            "2600 training iteraitions completed\n",
            "2700 training iteraitions completed\n",
            "2800 training iteraitions completed\n",
            "2900 training iteraitions completed\n",
            "3000 training iteraitions completed\n",
            "3100 training iteraitions completed\n",
            "3200 training iteraitions completed\n",
            "3300 training iteraitions completed\n",
            "3400 training iteraitions completed\n",
            "3500 training iteraitions completed\n",
            "3600 training iteraitions completed\n",
            "3700 training iteraitions completed\n",
            "3800 training iteraitions completed\n",
            "3900 training iteraitions completed\n",
            "4000 training iteraitions completed\n",
            "4100 training iteraitions completed\n",
            "4200 training iteraitions completed\n",
            "4300 training iteraitions completed\n",
            "4400 training iteraitions completed\n",
            "4500 training iteraitions completed\n",
            "4600 training iteraitions completed\n",
            "4700 training iteraitions completed\n",
            "4800 training iteraitions completed\n",
            "4900 training iteraitions completed\n",
            "5000 training iteraitions completed\n",
            "5000 1.9861\n",
            "5100 training iteraitions completed\n",
            "5200 training iteraitions completed\n",
            "5300 training iteraitions completed\n",
            "5400 training iteraitions completed\n",
            "5500 training iteraitions completed\n",
            "5600 training iteraitions completed\n",
            "5700 training iteraitions completed\n",
            "5800 training iteraitions completed\n",
            "5900 training iteraitions completed\n",
            "6000 training iteraitions completed\n",
            "6100 training iteraitions completed\n",
            "6200 training iteraitions completed\n",
            "6300 training iteraitions completed\n",
            "6400 training iteraitions completed\n",
            "6500 training iteraitions completed\n",
            "6600 training iteraitions completed\n",
            "6700 training iteraitions completed\n",
            "6800 training iteraitions completed\n",
            "6900 training iteraitions completed\n",
            "7000 training iteraitions completed\n",
            "7100 training iteraitions completed\n",
            "7200 training iteraitions completed\n",
            "7300 training iteraitions completed\n",
            "7400 training iteraitions completed\n",
            "7500 training iteraitions completed\n",
            "7600 training iteraitions completed\n",
            "7700 training iteraitions completed\n",
            "7800 training iteraitions completed\n",
            "7900 training iteraitions completed\n",
            "8000 training iteraitions completed\n",
            "8100 training iteraitions completed\n",
            "8200 training iteraitions completed\n",
            "8300 training iteraitions completed\n",
            "8400 training iteraitions completed\n",
            "8500 training iteraitions completed\n",
            "8600 training iteraitions completed\n",
            "8700 training iteraitions completed\n",
            "8800 training iteraitions completed\n",
            "8900 training iteraitions completed\n",
            "9000 training iteraitions completed\n",
            "9100 training iteraitions completed\n",
            "9200 training iteraitions completed\n",
            "9300 training iteraitions completed\n",
            "9400 training iteraitions completed\n",
            "9500 training iteraitions completed\n",
            "9600 training iteraitions completed\n",
            "9700 training iteraitions completed\n",
            "9800 training iteraitions completed\n",
            "9900 training iteraitions completed\n",
            "10000 training iteraitions completed\n",
            "10000 1.9237\n",
            "10100 training iteraitions completed\n",
            "10200 training iteraitions completed\n",
            "10300 training iteraitions completed\n",
            "10400 training iteraitions completed\n",
            "10500 training iteraitions completed\n",
            "10600 training iteraitions completed\n",
            "10700 training iteraitions completed\n",
            "10800 training iteraitions completed\n",
            "10900 training iteraitions completed\n",
            "11000 training iteraitions completed\n",
            "11100 training iteraitions completed\n",
            "11200 training iteraitions completed\n",
            "11300 training iteraitions completed\n",
            "11400 training iteraitions completed\n",
            "11500 training iteraitions completed\n",
            "11600 training iteraitions completed\n",
            "11700 training iteraitions completed\n",
            "11800 training iteraitions completed\n",
            "11900 training iteraitions completed\n",
            "12000 training iteraitions completed\n",
            "12100 training iteraitions completed\n",
            "12200 training iteraitions completed\n",
            "12300 training iteraitions completed\n",
            "12400 training iteraitions completed\n",
            "12500 training iteraitions completed\n",
            "12600 training iteraitions completed\n",
            "12700 training iteraitions completed\n",
            "12800 training iteraitions completed\n",
            "12900 training iteraitions completed\n",
            "13000 training iteraitions completed\n",
            "13100 training iteraitions completed\n",
            "13200 training iteraitions completed\n",
            "13300 training iteraitions completed\n",
            "13400 training iteraitions completed\n",
            "13500 training iteraitions completed\n",
            "13600 training iteraitions completed\n",
            "13700 training iteraitions completed\n"
          ]
        }
      ]
    }
  ]
}